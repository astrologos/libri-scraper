{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrologos/libri-scraper/blob/main/Public_Audiobook_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Public Audiobook Scraper\n",
        "The Public Audiobook Scraper downloads full audiobook MP3's from LibriVox.org.  It is meant to assist in building an labeled speech dataset for use in training neural Text-To-Speech systems in conjunction with Automatic Speech Recognition models."
      ],
      "metadata": {
        "id": "BUViiXtHVHsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "_aghvTzOZ1CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "nb_dir = '/content/drive/MyDrive/COLAB/Pub-Audiobook-Scraper'\n",
        "drive.mount('/content/drive/',force_remount=True)\n",
        "if not os.path.exists(nb_dir):\n",
        "    os.makedirs(nb_dir)\n",
        "os.chdir(nb_dir)\n",
        "print('Current path: ' + os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZFyfBaYZ2iQ",
        "outputId": "409bbe12-514d-4c1f-9f7e-b0baa26ae42c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Current path: /content/drive/MyDrive/COLAB/Pub-Audiobook-Scraper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Chromium and Selenium"
      ],
      "metadata": {
        "id": "vY0hfDhuWG6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Chromium\n",
        "%%shell\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n",
        "\n",
        "\n",
        "apt-get update\n",
        "apt-get install -qq chromium chromium-driver\n",
        "pip install -q selenium"
      ],
      "metadata": {
        "id": "7kZzA0G9PQ7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "SMPAnfV5W6VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System\n",
        "import sys\n",
        "import numpy.random as rand\n",
        "import time\n",
        "\n",
        "# IO\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "import zipfile\n",
        "\n",
        "# Web\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import parse_qs, urlparse, urlencode, urlunparse\n",
        "import requests"
      ],
      "metadata": {
        "id": "VI-sjQGbW7Uh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "8hxJ8QZkWutf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Util to modify search page number in URL\n",
        "def update_search_page(url, new_search_page):\n",
        "    parsed_url = urlparse(url)                                                  # Parse the URL\n",
        "    query_params = parse_qs(parsed_url.query)                                   # Get the query parameters\n",
        "    query_params['search_page'] = [str(new_search_page)]                        # Update the search_page parameter\n",
        "    updated_query_string = urlencode(query_params, doseq=True)                  # Construct the updated query string\n",
        "    updated_url = urlunparse(parsed_url._replace(query=updated_query_string))   # Construct the updated URL\n",
        "    return updated_url"
      ],
      "metadata": {
        "id": "eY995reAWucb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape links from LibriVox.org"
      ],
      "metadata": {
        "id": "CP-X8w9BWOv3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "# Set up Selenium\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.headless = True\n",
        "\n",
        "# Instantiate webdriver\n",
        "links = []\n",
        "wd = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "url = 'https://librivox.org/search?primary_key=1&search_category=language&search_page=1&search_form=get_results' \n",
        "max_pages = 500\n",
        "\n",
        "# Iterate through search pages\n",
        "for i in tqdm(range(max_pages)):\n",
        "    time.sleep(rand.randint(5, 5000) / 1000.0)                                  # Sleep a few seconds to avoid exposure / DOSing the website\n",
        "    wd.get(url)                                                                 # GET request\n",
        "    WebDriverWait(wd, 10).until(\n",
        "        EC.presence_of_element_located((By.CSS_SELECTOR, '.download-btn')))     # Load the webpage and wait for JavaScript to execute\n",
        "    html = wd.page_source                                                       # Get the fully rendered HTML\n",
        "    soup = BeautifulSoup(html, 'html.parser')                                   # Parse the HTML with BeautifulSoup\n",
        "    download_links = soup.find_all('a')\n",
        "\n",
        "    # Pull links\n",
        "    for link in download_links:\n",
        "        href = link.get('href')\n",
        "        if href.endswith('.zip'):\n",
        "            links.append(href)\n",
        "\n",
        "    # Update URL\n",
        "    url = update_search_page(url,i+2)\n",
        "\n",
        "# Print list of links when done\n",
        "_ = [print(i) for i in links]\n",
        "\n",
        "# Close the Selenium WebDriver\n",
        "wd.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save list of links"
      ],
      "metadata": {
        "id": "S31AsQ5vZm-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'audiolinks.pickle'\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(links, file)"
      ],
      "metadata": {
        "id": "CgwS13_mnq8x"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download, unzip and save mp3s"
      ],
      "metadata": {
        "id": "2xiodvhhaVMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create save folder\n",
        "output_directory = './mp3'\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Iterate through links\n",
        "for link in tqdm(links):    \n",
        "    # Download the ZIP file\n",
        "    response = requests.get(link)\n",
        "    file_path = os.path.join(output_directory, os.path.basename(link))\n",
        "    \n",
        "    # Write the ZIP file to disk\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Extract the ZIP file\n",
        "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_directory)\n",
        "\n",
        "    # Delete the downloaded ZIP file\n",
        "    os.remove(file_path)"
      ],
      "metadata": {
        "id": "FEDV30kIatzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0qQzc4Mn187"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-PR7haWn8gc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}